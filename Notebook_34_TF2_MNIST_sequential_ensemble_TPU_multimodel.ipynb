{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "34_TF2_MNIST_sequential_ensemble_TPU_multimodel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8Z73PnKh9PW"
      },
      "source": [
        "이번 노트북에서는 ensemble을 구성할때 서로다른 모델을 사용 가능한지에 대해서 살펴보자..\n",
        "\n",
        "Compare 도구를 사용하여 \"31_TF2_MNIST_sequential_ensemble_TPU.py\"와 \"34_TF2_MNIST_sequential_ensemble_TPU_multimodel.py\"를 비교하여 보자.\n",
        "\n",
        "때로는 ensemble 구성에서 서로다른 모델들을 사용하여 구성할때 좀더 효과적인 학습을 할때가 있다.\n",
        "\n",
        "따라서 향후에 연구자들이 부딪히게 되는 이슈중에서 ensemble로도 학습이 어려운 경우에는 아래의 code를 써보기 바란다.\n",
        "\n",
        "필자가 여러번 언급하지만 일반 연구자가 가볍게 시도해 볼수 있는 예제는 cifar100 정도인것 같다.\n",
        "\n",
        "이는 transfer learning 고급 과정에서 예제를 올리도록 하겠다.\n",
        "물론 cifar 100 이후버전의 예제도 준비 중이다. 이는 시간이 조금 나면 올리도록 하겠다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR6iHMmhoqZQ"
      },
      "source": [
        "Compare 도구를 사용하여 비교하면, 아래의 두개의 부분만 제외하고는 모두 동일함을 알수 있다.\n",
        "\n",
        "```\n",
        "model_2 = Sequential([\n",
        "    Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME',input_shape=(28, 28, 1)),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=256, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_3 = Sequential([\n",
        "    Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME',input_shape=(28, 28, 1)),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "```\n",
        "```\n",
        "with strategy.scope():\n",
        "    models = []\n",
        "    models.append(model)\n",
        "    models.append(model_2)\n",
        "    models.append(model_3)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8HIbh1_okyG",
        "outputId": "122cb76b-160f-4c88-b563-a5be32c8a4a1"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
        "from tensorflow.keras import Model, Sequential\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "import distutils\n",
        "if distutils.version.LooseVersion(tf.__version__) <= '2.0':\n",
        "    raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/fashion_mnist.ipynb')\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "## MNIST Dataset #########################################################\n",
        "# mnist = tf.keras.datasets.mnist\n",
        "# class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "##########################################################################\n",
        "\n",
        "## Fashion MNIST Dataset #################################################\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "##########################################################################\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()    \n",
        "\n",
        "# Change data type as float. If it is unt type, it might cause error \n",
        "X_train = X_train / 255.\n",
        "X_test  = X_test / 255.\n",
        "\n",
        "# in the case of Keras or TF2, type shall be [image_size, image_size, 1]\n",
        "# if it is RGB type, type shall be [image_size, image_size, 3]\n",
        "# For MNIST or Fashion MNIST, it need to reshape\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "    \n",
        "Y_train = to_categorical(Y_train, 10)\n",
        "Y_test = to_categorical(Y_test, 10)    \n",
        "    \n",
        "batch_size = 1000\n",
        "# 입력된 buffer_size만큼 data를 채우고 무작위로 sampling하여 새로운 data로 바꿉니다.\n",
        "# 완벽한 셔플링을 위해서는 데이터 세트의 전체 크기보다 크거나 같은 버퍼 크기가 필요합니다.\n",
        "# 만약 작은 데이터수보다 작은 buffer_size를 사용할경우,\n",
        "# 처음에 설정된 buffer_size만큼의 data안에서 임의의 셔플링이 발생합니다.\n",
        "shuffle_size = 100000\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (X_train, Y_train)).shuffle(shuffle_size).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (X_test, Y_test)).batch(batch_size)\n",
        "\n",
        "\"\"\"\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME', \n",
        "                                  input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPool2D(padding='SAME'))\n",
        "        \n",
        "    model.add(Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'))\n",
        "    model.add(MaxPool2D(padding='SAME'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation=tf.nn.relu))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(10, activation=tf.nn.softmax))\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "\"\"\"\n",
        "model = Sequential([\n",
        "    Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME',input_shape=(28, 28, 1)),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_2 = Sequential([\n",
        "    Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME',input_shape=(28, 28, 1)),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=256, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_3 = Sequential([\n",
        "    Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME',input_shape=(28, 28, 1)),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "# checkpoints = []\n",
        "# for m in range(num_models):\n",
        "#     checkpoints.append(tf.train.Checkpoint(cnn=models[m]))\n",
        "\n",
        "with strategy.scope():\n",
        "    models = []\n",
        "    models.append(model)\n",
        "    models.append(model_2)\n",
        "    models.append(model_3)\n",
        "    \n",
        "    def loss_fn(model, images, labels):\n",
        "\n",
        "        logits = model(images, training=True)\n",
        "        loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(\n",
        "            y_pred=logits, y_true=labels, from_logits=True))\n",
        "        return loss   \n",
        "\n",
        "    def grad(model, images, labels):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = loss_fn(model, images, labels)\n",
        "        return tape.gradient(loss, model.variables)\n",
        "\n",
        "    def evaluate(models, images, labels):\n",
        "        predictions = np.zeros_like(labels)\n",
        "        for model in models:\n",
        "            logits = model(images, training=False)\n",
        "            predictions += logits\n",
        "        correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        return accuracy\n",
        "\n",
        "    def train(model, images, labels):\n",
        "        grads = grad(model, images, labels)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = 0.\n",
        "    test_loss = 0.\n",
        "    train_accuracy = 0.\n",
        "    test_accuracy = 0.\n",
        "    train_step = 0\n",
        "    test_step = 0    \n",
        "    \n",
        "    for images, labels in train_ds:\n",
        "        for model in models:\n",
        "            train(model, images, labels)\n",
        "            # grads = grad(model, images, labels)                \n",
        "            # optimizer.apply_gradients(zip(grads, model.variables))\n",
        "            loss = loss_fn(model, images, labels)\n",
        "            train_loss += loss / num_models\n",
        "        acc = evaluate(models, images, labels)\n",
        "        train_accuracy += acc\n",
        "        train_step += 1\n",
        "    train_loss = train_loss / train_step\n",
        "    train_accuracy = train_accuracy / train_step\n",
        "    \n",
        "    for test_images, test_labels in test_ds:\n",
        "        for model in models:\n",
        "            loss = loss_fn(model, test_images, test_labels)\n",
        "            test_loss += loss\n",
        "        acc = evaluate(models, test_images, test_labels)        \n",
        "        test_accuracy += acc\n",
        "        test_step += 1\n",
        "    test_loss = test_loss / test_step\n",
        "    test_accuracy = test_accuracy / test_step\n",
        "    \n",
        "    \"\"\"\n",
        "    print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(train_loss), \n",
        "          'train accuracy = ', '{:.4f}'.format(train_accuracy), \n",
        "          'test accuracy = ', '{:.4f}'.format(test_accuracy))\n",
        "    \"\"\"\n",
        "    template = 'epoch: {:>5,d}, loss: {:>2.4f}, accuracy: {:>2.3f} %, test loss: {:>2.4f}, test accuracy: {:>2.3f} %'\n",
        "    print (template.format(epoch+1,\n",
        "                         train_loss,\n",
        "                         train_accuracy*100,\n",
        "                         test_loss,\n",
        "                         test_accuracy*100))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "WARNING:tensorflow:TPU system grpc://10.78.242.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.78.242.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.78.242.250:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.78.242.250:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU')]\n",
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch:     1, loss: 1.8445, accuracy: 66.652 %, test loss: 5.1346, test accuracy: 78.450 %\n",
            "epoch:     2, loss: 1.6846, accuracy: 80.360 %, test loss: 4.9908, test accuracy: 81.100 %\n",
            "epoch:     3, loss: 1.6387, accuracy: 83.152 %, test loss: 4.9138, test accuracy: 83.640 %\n",
            "epoch:     4, loss: 1.6162, accuracy: 86.580 %, test loss: 4.8497, test accuracy: 86.410 %\n",
            "epoch:     5, loss: 1.6031, accuracy: 88.000 %, test loss: 4.8278, test accuracy: 87.030 %\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
